{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AziizBg/g-retriever/blob/main/Copie_de_g_retriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AziizBg/g-retriever.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw83NooFPckb",
        "outputId": "4cedad82-3b8b-48d1-bdf2-012499925c6c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'g-retriever'...\n",
            "remote: Enumerating objects: 126, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 126 (delta 35), reused 45 (delta 18), pack-reused 48 (from 1)\u001b[K\n",
            "Receiving objects: 100% (126/126), 50.06 MiB | 18.90 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd g-retriever/"
      ],
      "metadata": {
        "id": "iM8oLpi1g9By",
        "outputId": "63f919f2-a2f3-49b8-fa1a-7eaff280fda0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/g-retriever\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "\n",
        "# Install the compatible PyTorch version\n",
        "!pip install torch==2.2.2 torchvision==0.17.2  torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zyi-YTaofJdf",
        "outputId": "299afd17-70fd-4199-9a7a-1e75c3dc3af3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.2.2\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.2.2%2Bcu118-cp311-cp311-linux_x86_64.whl (819.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m819.2/819.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.17.2\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.17.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.2.2\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.2%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.7.0.84 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m728.5/728.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.19.3 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.19.3-py3-none-manylinux1_x86_64.whl (135.3 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m135.3/135.3 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2.0 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.2) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.2) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.19.3 nvidia-nvtx-cu11-11.8.86 torch-2.2.2+cu118 torchaudio-2.2.2+cu118 torchvision-0.17.2+cu118 triton-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyG + dependencies for torch 2.0.1 + cu118\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install torch_geometric\n",
        "\n",
        "# Your other dependencies\n",
        "!pip install peft pandas ogb transformers wandb sentencepiece datasets pcst_fast gensim scipy==1.12 protobuf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3wjz0QXTf1mT",
        "outputId": "7d9f92e8-0c84-40f3-ef7f-8dd77d6ad817"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/pyg_lib-0.4.0%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch_scatter in /usr/local/lib/python3.11/dist-packages (2.1.2+pt21cu118)\n",
            "Collecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.18%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.3%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (886 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m886.5/886.5 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n",
            "Installing collected packages: torch_spline_conv, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.4.0+pt20cu118 torch_cluster-1.6.3+pt20cu118 torch_sparse-0.6.18+pt20cu118 torch_spline_conv-1.2.2+pt20cu118\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting pcst_fast\n",
            "  Downloading pcst_fast-1.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting scipy==1.12\n",
            "  Downloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.4)\n",
            "Collecting numpy<1.29.0,>=1.22.4 (from scipy==1.12)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.2.2+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.6.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.31.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.6.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.4.0)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.28.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Collecting pybind11>=2.1.0 (from pcst_fast)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (8.7.0.84)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.8.86)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Downloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pcst_fast-1.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: pybind11, numpy, littleutils, scipy, pcst_fast, outdated, gensim, ogb\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 littleutils-0.2.4 numpy-1.26.4 ogb-1.3.6 outdated-0.2.2 pcst_fast-1.0.10 pybind11-2.13.6 scipy-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch-scatter -y\n",
        "\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.2.2+cu118.html\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Q7vfT_5EjjB7",
        "outputId": "244ad0cb-ce92-494e-8a36-bb8df502a9b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch-scatter 2.1.2+pt21cu118\n",
            "Uninstalling torch-scatter-2.1.2+pt21cu118:\n",
            "  Successfully uninstalled torch-scatter-2.1.2+pt21cu118\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.2+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu118/torch_scatter-2.1.2%2Bpt22cu118-cp311-cp311-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt22cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # expla_graphs\n",
        "# !python -m src.dataset.preprocess.expla_graphs\n",
        "# !python -m src.dataset.expla_graphs"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z2aTldUJmH2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 洫ｹ Nettoyage prﾃｩalable\n",
        "!pip uninstall -y torch torchao torch-geometric torch-sparse torch-scatter torch-cluster torch-spline-conv transformers\n",
        "\n",
        "# 笨 Installer la version compatible de PyTorch\n",
        "!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# 笨 Installer les dﾃｩpendances PyG (torch-geometric et associﾃｩes)\n",
        "!pip install torch-geometric -f https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
        "\n",
        "# 笨 Installer transformers\n",
        "!pip install transformers==4.39.3\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmceXdLIMSRd",
        "outputId": "9a08a16e-ffaa-4e47-a7ba-c7cbf8228585"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.2.2+cu118\n",
            "Uninstalling torch-2.2.2+cu118:\n",
            "  Successfully uninstalled torch-2.2.2+cu118\n",
            "Found existing installation: torchao 0.10.0\n",
            "Uninstalling torchao-0.10.0:\n",
            "  Successfully uninstalled torchao-0.10.0\n",
            "Found existing installation: torch-geometric 2.6.1\n",
            "Uninstalling torch-geometric-2.6.1:\n",
            "  Successfully uninstalled torch-geometric-2.6.1\n",
            "Found existing installation: torch-sparse 0.6.18+pt20cu118\n",
            "Uninstalling torch-sparse-0.6.18+pt20cu118:\n",
            "  Successfully uninstalled torch-sparse-0.6.18+pt20cu118\n",
            "Found existing installation: torch_scatter 2.1.2+pt22cu118\n",
            "Uninstalling torch_scatter-2.1.2+pt22cu118:\n",
            "  Successfully uninstalled torch_scatter-2.1.2+pt22cu118\n",
            "Found existing installation: torch-cluster 1.6.3+pt20cu118\n",
            "Uninstalling torch-cluster-1.6.3+pt20cu118:\n",
            "  Successfully uninstalled torch-cluster-1.6.3+pt20cu118\n",
            "Found existing installation: torch-spline-conv 1.2.2+pt20cu118\n",
            "Uninstalling torch-spline-conv-1.2.2+pt20cu118:\n",
            "  Successfully uninstalled torch-spline-conv-1.2.2+pt20cu118\n",
            "Found existing installation: transformers 4.51.3\n",
            "Uninstalling transformers-4.51.3:\n",
            "  Successfully uninstalled transformers-4.51.3\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.1.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m547.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.16.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.1.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2025.3.2)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
            "Installing collected packages: triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.17.2+cu118\n",
            "    Uninstalling torchvision-0.17.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.17.2+cu118\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.2.2+cu118\n",
            "    Uninstalling torchaudio-2.2.2+cu118:\n",
            "      Successfully uninstalled torchaudio-2.2.2+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "peft 0.15.2 requires transformers, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.1.0+cu118 torchaudio-2.1.0+cu118 torchvision-0.16.0+cu118 triton-2.1.0\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
            "Collecting torch-geometric\n",
            "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Collecting transformers==4.39.3\n",
            "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.3)\n",
            "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (2025.4.26)\n",
            "Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.39.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.15.2 transformers-4.39.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/datasets/rmanluo/RoG-webqsp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdAM3vpSRIa3",
        "outputId": "3a8d5c12-e873-4ef1-c89d-9a30cea4d14f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RoG-webqsp'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Total 30 (delta 0), reused 0 (delta 0), pack-reused 30 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (30/30), 4.15 KiB | 530.00 KiB/s, done.\n",
            "Filtering content: 100% (5/5), 494.20 MiB | 57.83 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/g-retriever/RoG-webqsp/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afgrLUI5WrCG",
        "outputId": "2b1eefce-b1d6-4793-8c5d-9a7c053a111c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test-00000-of-00002-9ee8d68f7d951e1f.parquet\n",
            "test-00001-of-00002-773a7b8213e159f5.parquet\n",
            "train-00000-of-00002-d810a36ed97bc2cc.parquet\n",
            "train-00001-of-00002-e53244e71082a392.parquet\n",
            "validation-00000-of-00001-6ee6adc5b154643a.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# webqsp\n",
        "!python -m src.dataset.preprocess.webqsp\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja8CXu2ELc6c",
        "outputId": "533ecb7e-6879-4b6e-972a-e568b8197cbc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/libpyg.so: undefined symbol: _ZNK3c106SymInt13toSymNodeImplEv\n",
            "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
            "<frozen runpy>:128: RuntimeWarning: 'src.dataset.preprocess.webqsp' found in sys.modules after import of package 'src.dataset.preprocess', but prior to execution of 'src.dataset.preprocess.webqsp'; this may result in unpredictable behaviour\n",
            "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "100% 100/100 [00:07<00:00, 13.40it/s]\n",
            "Loading dataset...\n",
            "inherit model weights from sentence-transformers/all-roberta-large-v1\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 650/650 [00:00<00:00, 3.66MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 1.42G/1.42G [00:12<00:00, 117MB/s]\n",
            "tokenizer_config.json: 100% 328/328 [00:00<00:00, 2.54MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 9.82MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 69.3MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 22.5MB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 2.17MB/s]\n",
            "Encoding questions...\n",
            "100% 74/74 [00:04<00:00, 17.98it/s]\n",
            "Encoding graphs (processing 100 of 4700)...\n",
            "  3% 3/100 [00:20<09:53,  6.12s/it, processed=3, skipped=0]\n",
            "Error processing graph 3: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 14.74 GiB of which 5.85 GiB is free. Process 30873 has 8.89 GiB memory in use. Of the allocated memory 7.68 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            " 99% 99/100 [14:03<00:08,  8.52s/it, processed=99, skipped=0]\n",
            "\n",
            "Completed! Processed: 99, Skipped: 0, Error: 1\n",
            "# train samples:  80\n",
            "# val samples:  10\n",
            "# test samples:  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.dataset.webqsp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-YKQJrrBeKV",
        "outputId": "fec6aaca-43ae-4a58-bc40-48c5eb03e9d8",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/libpyg.so: undefined symbol: _ZNK3c106SymInt13toSymNodeImplEv\n",
            "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
            "<frozen runpy>:128: RuntimeWarning: 'src.dataset.webqsp' found in sys.modules after import of package 'src.dataset', but prior to execution of 'src.dataset.webqsp'; this may result in unpredictable behaviour\n",
            "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Index: 0\n",
            "id: 0\n",
            "question: Question: what is the name of justin bieber brother\n",
            "Answer: \n",
            "label: jaxon bieber\n",
            "graph: Data(x=[15, 1024], edge_index=[2, 20], edge_attr=[20, 1024], num_nodes=15)\n",
            "desc: node_id,node_attr\n",
            "15,justin bieber\n",
            "151,pattie mallette\n",
            "173,m.0gxnp5d\n",
            "286,english language\n",
            "294,jaxon bieber\n",
            "346,yves bole\n",
            "356,jeremy bieber\n",
            "452,jazmyn bieber\n",
            "545,m.0wfn4pm\n",
            "551,m.0gxnnwp\n",
            "610,m.0gxnp5x\n",
            "933,m.0gxnnwc\n",
            "1032,this is justin bieber\n",
            "1359,m.0129jzth\n",
            "1567,m.0gxnp5m\n",
            "\n",
            "src,edge_attr,dst\n",
            "346,people.person.languages,286\n",
            "1032,film.film.language,286\n",
            "346,influence.influence_node.influenced_by,15\n",
            "151,people.person.children,15\n",
            "294,people.person.parents,356\n",
            "545,people.sibling_relationship.sibling,151\n",
            "294,people.person.sibling_s,551\n",
            "15,base.popstra.celebrity.hangout,610\n",
            "933,people.sibling_relationship.sibling,452\n",
            "1359,people.sibling_relationship.sibling,346\n",
            "151,people.person.sibling_s,545\n",
            "551,people.sibling_relationship.sibling,294\n",
            "346,people.person.sibling_s,1359\n",
            "15,people.person.sibling_s,933\n",
            "15,base.popstra.celebrity.hangout,173\n",
            "452,people.person.sibling_s,933\n",
            "933,people.sibling_relationship.sibling,15\n",
            "15,people.person.sibling_s,551\n",
            "15,base.popstra.celebrity.hangout,1567\n",
            "551,people.sibling_relationship.sibling,15\n",
            "\n",
            "# train: 80\n",
            "# val: 10\n",
            "# test: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5L0KBqHlg0p5",
        "outputId": "8dc6bd16-4a6e-44e5-e5d3-df0be458f1de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.39.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.39.3\n",
            "    Uninstalling transformers-4.39.3:\n",
            "      Successfully uninstalled transformers-4.39.3\n",
            "Successfully installed tokenizers-0.21.1 transformers-4.51.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch-scatter -y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu118.html"
      ],
      "metadata": {
        "id": "PAvieM5PhJYr",
        "outputId": "eeb6a544-145a-4e4d-90b3-fb1a70a588d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Using cached https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_scatter-2.1.2%2Bpt21cu118-cp311-cp311-linux_x86_64.whl (10.2 MB)\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "id": "h75lWqOlVGN_",
        "outputId": "e0fb9913-c7bf-446e-eb62-e26fa6c86645",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects:  11% (1/9)\u001b[K\rremote: Counting objects:  22% (2/9)\u001b[K\rremote: Counting objects:  33% (3/9)\u001b[K\rremote: Counting objects:  44% (4/9)\u001b[K\rremote: Counting objects:  55% (5/9)\u001b[K\rremote: Counting objects:  66% (6/9)\u001b[K\rremote: Counting objects:  77% (7/9)\u001b[K\rremote: Counting objects:  88% (8/9)\u001b[K\rremote: Counting objects: 100% (9/9)\u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 5 (delta 3), reused 5 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  20% (1/5)\rUnpacking objects:  40% (2/5)\rUnpacking objects:  60% (3/5)\rUnpacking objects:  80% (4/5)\rUnpacking objects: 100% (5/5)\rUnpacking objects: 100% (5/5), 1.13 KiB | 1.13 MiB/s, done.\n",
            "From https://github.com/AziizBg/g-retriever\n",
            "   ddfe579..9ddd3bd  main       -> origin/main\n",
            "Updating ddfe579..9ddd3bd\n",
            "Fast-forward\n",
            " src/model/graph_llm.py | 70 \u001b[32m++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " 1 file changed, 70 insertions(+)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train2.py --dataset webqsp"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rcEjm2-xJw1O",
        "outputId": "8583bf3c-ec97-4502-f5eb-05004138f2cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-16 09:47:50.027631: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747388870.048033   20717 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747388870.055283   20717 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-16 09:47:50.076446: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/libpyg.so: undefined symbol: _ZNK3c106SymInt13toSymNodeImplEv\n",
            "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory. Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "Namespace(model_name='graph_llm', project='project_g_retriever', seed=0, dataset='webqsp', lr=1e-05, wd=0.05, patience=2, batch_size=8, grad_steps=2, num_epochs=10, warmup_epochs=1, eval_batch_size=16, llm_model_name='7b', llm_model_path='', llm_frozen='True', llm_num_virtual_tokens=10, output_dir='output', max_txt_len=512, max_new_tokens=32, max_memory=[80, 80], gnn_model_name='gt', gnn_num_layers=4, gnn_in_dim=1024, gnn_hidden_dim=1024, gnn_num_heads=4, gnn_dropout=0.0)\n",
            "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Graph file does not exist at index 3\n",
            "Train Dataset Sample:\n",
            "dict_keys(['id', 'question', 'label', 'graph', 'desc'])\n",
            "Loading LLAMA\n",
            "Loading checkpoint shards: 100% 2/2 [00:12<00:00,  6.23s/it]\n",
            "Freezing LLAMA!\n",
            "Finish loading LLAMA!\n",
            "trainable params: 28347904 || all params: 2808031744 || trainable%: 1.009529328169874\n",
            "  0% 0/790 [00:00<?, ?it/s]NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2755 > 2048). Running this sequence through the model will result in indexing errors\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "Epoch 0/10 - Train Loss: 0.0000\n",
            "Epoch 0/10 - Val Loss: nan\n",
            "Warning: No best epoch found.\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "NaN/Inf in loss, skipping batch\n",
            "Epoch 1/10 - Train Loss: 0.0000\n",
            "Epoch 1/10 - Val Loss: nan\n",
            "Warning: No best epoch found.\n",
            "Early stopping at epoch 1\n",
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "Saving inference to: output/webqsp/model_name_graph_llm_llm_model_name_phi2_llm_frozen_True_max_txt_len_256_max_new_tokens_16_gnn_model_name_gt_patience_2_num_epochs_10_seed0.csv\n",
            "Loading checkpoint from output/webqsp/model_name_graph_llm_llm_model_name_phi2_llm_frozen_True_max_txt_len_256_max_new_tokens_16_gnn_model_name_gt_patience_2_num_epochs_10_seed0_checkpoint_best.pth.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/g-retriever/train2.py\", line 186, in <module>\n",
            "    main(args)\n",
            "  File \"/content/g-retriever/train2.py\", line 167, in main\n",
            "    model = _reload_best_model(model, args)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/g-retriever/src/utils/ckpt.py\", line 52, in _reload_best_model\n",
            "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 986, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 435, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 416, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "                     ^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'output/webqsp/model_name_graph_llm_llm_model_name_phi2_llm_frozen_True_max_txt_len_256_max_new_tokens_16_gnn_model_name_gt_patience_2_num_epochs_10_seed0_checkpoint_best.pth'\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[1;34mwandb\u001b[0m: \u001b[1mwandb sync /content/g-retriever/wandb/offline-run-20250516_094810-z2ea7wnl\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/offline-run-20250516_094810-z2ea7wnl/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZFJMEWHwOPb4",
        "outputId": "796e0013-b602-49d2-91e9-680a087c0455",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}